{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97213025",
   "metadata": {},
   "source": [
    "# Pandas Input/output\n",
    "\n",
    "Can read: `.csv`, `.json`, `.xlsx`, `.parquet`, `.db`, `.hdf`, ...\n",
    "\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/io.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b7567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8ba03",
   "metadata": {},
   "source": [
    "### Our Data\n",
    "\n",
    "- `attacks.csv` from Kaggle\n",
    "- `github_pulls.json` from GitHub API\n",
    "- `2021_Accidentalidad.xlsx` from Datos Abiertos Ayuntamiento de Madrid\n",
    "- `test.parquet` from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d1ad7b",
   "metadata": {},
   "source": [
    "__.CSV Files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c307aea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/attacks.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v7/b_w4v4wj48707qytc6tfxtjw0000gn/T/ipykernel_2783/2848278981.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Raw \"Comma\"-Separated Values file (a.k.a.: CSV file) Separo los valores por comas, y cada fila es una linea.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./datasets/attacks.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/attacks.csv'"
     ]
    }
   ],
   "source": [
    "# Raw \"Comma\"-Separated Values file (a.k.a.: CSV file) Separo los valores por comas, y cada fila es una linea. \n",
    "\n",
    "with open('./datasets/attacks.csv') as f:\n",
    "    lines = f.readlines()\n",
    "lines\n",
    "\n",
    "#El type del csv es una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aca4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import .CSV \n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#\n",
    "#NaN.\n",
    "df_csv = pd.read_csv('./datasets/attacks.csv', enconding = 'iso8859_15')#convierte el objeto en un objeto de pandas.#Para la ruta, parte desde el punto donde esta el archivo.Un punto es ese directorio, dos es sacame. #El iso te da un formato del codigo para los acaracteres.\n",
    "df_csv.head()#muestra un parte del dataframe. Le puede pones un valor y que me devuelva las filas indicadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282b3f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the dataset\n",
    "\n",
    "print(df_csv.shape)# te deice las fila y las columnas.\n",
    "print(df_csv.info())#información sobre las columnas(número, nombres, tipos de datos...)\n",
    "#df_csv_short = df_csv[['Date', 'Type', 'Country', 'Injury']]#esto es crear un subset de nuestros datos. Entonces nos devuelve los datos de estas columnas.Genero un nuevo dataframe. \n",
    "#df_csv_short = df_csv_short.dropna(subset=['Country'])#Aquí le quito todas las filas que no tengas el valor \"country\".\n",
    "#print(df_csv_short.shape)\n",
    "#print(df_csv_short.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new .csv file\n",
    "\n",
    "df_csv_short.to_csv('./datasets/shark_attacks_short.csv', sep=';')#Exportar un nuevo CSV separado por ;. Pero los csv se tienen que separar por comas para q sean leidos bien. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d165b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_s = pd.read_csv('./datasets/shark_attacks_short.csv', sep=';')#Importo, pero en vez de comas, por punto y coma. Parser es un error al analizar un archivo. \n",
    "\n",
    "df_csv_s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d65d5b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be646d",
   "metadata": {},
   "source": [
    "__.JSON Files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf608ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Raw JavaScript Object Notation file (a.k.a.: JSON file)\n",
    "\n",
    "import json\n",
    "\n",
    "with open('./datasets/github_pulls.json', encoding=\"utf8\") as f:\n",
    "    json_file = json.load(f)\n",
    "\n",
    "json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd98df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import .JSON #es una lista que tiene diccionarios. \n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.io.json.read_json.html#pandas.io.json.read_json\n",
    "\n",
    "df_json = pd.read_json('./datasets/github_pulls.json')\n",
    "df_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the dataset\n",
    "\n",
    "print(df_json.shape)\n",
    "print(df_json.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a49524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict flatten\n",
    "\n",
    "df_json_new = list(df_json['_links'])# cojo el diccionario de esa columna. Me devuelve una serie. \n",
    "df_json_new = pd.DataFrame(df_json_new)# lo convierto en dataframe. \n",
    "df_json_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb08ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new .JSON file\n",
    "\n",
    "df_json_new.to_json('./datasets/github_pulls_new.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0862fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae360767",
   "metadata": {},
   "source": [
    "__.XLSX Files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional libraries for Excel files\n",
    "\n",
    "#!conda install -y xlrd\n",
    "#!conda install -y openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5222d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import .XLSX \n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html\n",
    "\n",
    "df_excel = pd.read_excel('./datasets/2021_Accidentalidad.xlsx', sheet_name='Accidentes_2021')\n",
    "df_excel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6888056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the dataset\n",
    "\n",
    "print(df_excel.shape)\n",
    "print(df_excel.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630bc08e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04949dad",
   "metadata": {},
   "source": [
    "__.PARQUET Files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional library for Parquet files\n",
    "#es un tipo de dato optimizado para Big Dta. \n",
    "\n",
    "#!conda install -c conda-forge pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a51ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import .PARQUET (column-oriented data storage format with schema)\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html\n",
    "\n",
    "df_parquet = pd.read_parquet('./datasets/test.parquet')\n",
    "df_parquet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5abead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the dataset (https://www.kaggle.com/dschettler8845/recsys-2020-ecommerce-dataset)\n",
    "\n",
    "print(df_parquet.shape)\n",
    "print(df_parquet.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a4fdc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0e834",
   "metadata": {},
   "source": [
    "__SQL Files...in the next episode...__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ironhack]",
   "language": "python",
   "name": "conda-env-ironhack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
